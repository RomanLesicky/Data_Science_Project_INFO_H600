{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a996366",
   "metadata": {},
   "source": [
    "---\n",
    "# INFO-H600 - Computing Foundations of Data Sciences\n",
    "\n",
    "## Team 14 : \n",
    "\n",
    "Roman Lešický, Theo Abraham, Kevin Straatman, Lara Hansen, Grégoire Van den Eynde and Nicolas Roux\n",
    "\n",
    "Version of python : 3.11.14 | packaged by conda-forge \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055e72d",
   "metadata": {},
   "source": [
    "# Library:\n",
    "\n",
    "###### The download environment.txt is present within the Github repository of the project https://github.com/RomanLesicky/Data_Science_Project_INFO_H600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba5b28",
   "metadata": {},
   "source": [
    "### How path's are handled in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\roman\\Desktop\\Master - ULB\\2nd year\\Q1\\Intro Data Sc\\Data_Science_Project_INFO_H600\n",
      "Data dir: C:\\Users\\roman\\Desktop\\Master - ULB\\2nd year\\Q1\\Intro Data Sc\\Data_Science_Project_INFO_H600\\data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path # We are using the pathlib library for our paths \n",
    "\n",
    "# The way the code works is that we first locate the project's root\n",
    "project_root = Path.cwd().resolve()\n",
    "\n",
    "# Then we make a variable which hall be used as our data directory path which is sued for everyone in this project \n",
    "# For steps 2 till 5 included.\n",
    "\n",
    "data_dir = project_root / \"data\" \n",
    "\n",
    "# Simple print for as a sanity check\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Data dir:\", data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7855a1",
   "metadata": {},
   "source": [
    "### Rest of the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions \n",
    "\n",
    "#! To be continued ofc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e696d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c7f84",
   "metadata": {},
   "source": [
    "# Overview of the project :\n",
    "\n",
    "# `WIP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170d29b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be0f2a",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "\n",
    "`TBD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ea973",
   "metadata": {},
   "source": [
    "# DO NOT FORGET TO DO THE DOCUMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc0d5f",
   "metadata": {},
   "source": [
    "# `Pas oublier de justifier avec le cours + documentaion official + re-write a part`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659cd31",
   "metadata": {},
   "source": [
    "### 1.0 Set-up of the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69471d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int this cell we initialise a SparkSession, which can be reused.\n",
    "# An important part of this code is that indicating to Spark to run all available CPU cores, for each task utilizing Spark.\n",
    "# Therefore, the use of the code has been warned that when they are running cell which are Spark related this will utilize their whole CPU.\n",
    "# The reason for doing this is that it gives us parallelism without needing a proper cluster\n",
    "spark = (SparkSession.builder.appName(\"MillionPlaylistProject\") .master(\"local[*]\").getOrCreate())\n",
    "\n",
    "spark  # Just for postery we display the session "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffecdec",
   "metadata": {},
   "source": [
    "### 1.1 Reading JSON slices into raw DataFrames using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2e390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m SLICES_DEV = \u001b[32m5\u001b[39m       \u001b[38;5;66;03m# ≈ 5k playlists\u001b[39;00m\n\u001b[32m     25\u001b[39m SLICES_MEDIUM = \u001b[32m50\u001b[39m   \u001b[38;5;66;03m# ≈ 50k playlists\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice_start_key\u001b[39m(path: \u001b[43mPath\u001b[49m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    Extract the numeric start index from filenames like 'mpd.slice.1000-1999.json'\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    so that we sort slices in the correct numeric order.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m     name = path.name                       \u001b[38;5;66;03m# e.g. 'mpd.slice.1000-1999.json'\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "This is an important part of the project which needs to be addressed. \n",
    "\n",
    "The question \"How many sliced files do we want to read?\" needs to be asked since this determined the trade-off between scalability and practical runtime.\n",
    "Essentially, the answer to that question is having a sufficient amount of data that our metrics will be good whilst not calculating until forever. \n",
    "\n",
    "As a group we have decide to hardcode the value 5 for demonstration purposes, this means that we shall ony use mpd.slice.0 to 4999 so only about 5k playlists. \n",
    "The soul reason for this specific value is that it small enough to run very fast and yet demonstrate that the pipeline works. \n",
    "Additionally the use can adapt this number via the global variable `NUMBER_OF_SLICES`, but they shall keep in mind the that they are using all the cores of their CPU for this. \n",
    "\n",
    "That being said, for practical reasons which concern task's 3 and 4 (5 too) we shall use a dataset that contains 50 slices meaning 50 thousand playlists. \n",
    "This value does provides enough data to obtain stable aggregate statistics and similarity scores while keeping computation times manageable on a single machine.\n",
    "\n",
    "Here we do not use a randomized method to chose the slices, since the data at hand is not ordered nor are we worried with a certain bias since we shall be using the 50k \n",
    "version for the actual metric determination. \n",
    "\"\"\"\n",
    "\n",
    "# So this global variable is to be changed if one desires for a higher number of slices \n",
    "NUMBER_OF_SLICES = 5  \n",
    "\n",
    "# This is the file path to the original Million Playlist Dataset to be used only in this Task.\n",
    "# This dataset will never be published to github since it's under the .gitignore file. \n",
    "data_dir = project_root / \"data\" \n",
    "\n",
    "def slice_start_key(path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Extract the numeric start index from filenames like 'mpd.slice.1000-1999.json'\n",
    "    so that we sort slices in the correct numeric order.\n",
    "    \"\"\"\n",
    "    name = path.name                       # e.g. 'mpd.slice.1000-1999.json'\n",
    "    middle = name.split('.')[2]            # '1000-1999'\n",
    "    start_str = middle.split('-')[0]       # '1000'\n",
    "    return int(start_str)\n",
    "\n",
    "#! Need to justify this via documentation I GUESS \n",
    "\n",
    "# List and sort all slice files numerically\n",
    "all_slices = sorted(data_dir.glob(\"mpd.slice.*.json\"),key=slice_start_key)\n",
    "\n",
    "# Effective number of slices we will use\n",
    "num_slices = min(NUMBER_OF_SLICES, len(all_slices))\n",
    "\n",
    "input_paths = [str(p) for p in all_slices[:num_slices]]\n",
    "\n",
    "print(f\"\\nNUMBER_OF_SLICES = {NUMBER_OF_SLICES} → actually using {num_slices} slice file(s):\")\n",
    "for p in input_paths:\n",
    "    print(\"  \", p)\n",
    "\n",
    "# Read the selected slice files as a single DataFrame.\n",
    "# Each file has the structure: {\"info\": {...}, \"playlists\": [ {...}, {...}, ... ]}\n",
    "playlists_raw_df = (\n",
    "    spark.read\n",
    "    .option(\"multiLine\", True)  # MPD JSON files are multi-line JSON, not line-delimited\n",
    "    .json(input_paths)\n",
    ")\n",
    "\n",
    "print(\"\\nSchema of the raw JSON DataFrame:\")\n",
    "playlists_raw_df.printSchema()\n",
    "\n",
    "print(\"\\nExample row (one JSON file):\")\n",
    "playlists_raw_df.show(1, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02910b89",
   "metadata": {},
   "source": [
    "### 1.2 Flattening pipeline \n",
    "\n",
    "# Add more infos here cuz this is a bit meager \n",
    "\n",
    "> Note: For MODE=\"dev\"/\"medium\" (5 or 50 slices) we rely on Spark's built-in schema inference. If we processed all 35 GB or ran on a cluster, we would define an explicit StructType schema to avoid an extra pass over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! Change the whole F.col sine that is the default name of GPT \n",
    "\n",
    "### 1.2 Flattening pipeline (slices → playlists → playlist–track rows)\n",
    "\n",
    "# 1) Flatten `playlists`: one row per playlist\n",
    "playlists_df = playlists_raw_df.select(\n",
    "    functions.explode(\"playlists\").alias(\"playlist\")  # explode the array of playlists per file\n",
    ")\n",
    "\n",
    "# Extract playlist-level fields we care about.\n",
    "# We keep the `tracks` array for the next step.\n",
    "playlists_flat_df = playlists_df.select(\n",
    "    functions.col(\"playlist.pid\").alias(\"pid\"),\n",
    "    functions.col(\"playlist.name\").alias(\"name\"),\n",
    "    functions.col(\"playlist.collaborative\").alias(\"collaborative\"),\n",
    "    functions.col(\"playlist.modified_at\").alias(\"modified_at\"),\n",
    "    functions.col(\"playlist.num_tracks\").alias(\"num_tracks\"),\n",
    "    functions.col(\"playlist.num_albums\").alias(\"num_albums\"),\n",
    "    functions.col(\"playlist.num_followers\").alias(\"num_followers\"),\n",
    "    functions.col(\"playlist.duration_ms\").alias(\"duration_ms\"),\n",
    "    functions.col(\"playlist.tracks\").alias(\"tracks\")  # still an array ofunctions track structs\n",
    ")\n",
    "\n",
    "print(\"Schema of playlist-level table:\")\n",
    "playlists_flat_df.printSchema()\n",
    "print(\"\\nExample playlists:\")\n",
    "playlists_flat_df.show(3, truncate=False)\n",
    "\n",
    "# 2) Flatten `tracks`: one row per (playlist, track)\n",
    "playlist_track_df = playlists_flat_df.select(\n",
    "    functions.col(\"pid\"),\n",
    "    functions.col(\"name\").alias(\"playlist_name\"),\n",
    "    functions.col(\"num_tracks\"),\n",
    "    functions.col(\"num_albums\"),\n",
    "    functions.col(\"num_followers\"),\n",
    "    functions.col(\"modified_at\"),\n",
    "    functions.col(\"duration_ms\").alias(\"playlist_duration_ms\"),\n",
    "    functions.explode(\"tracks\").alias(\"track\")   # explode the tracks array\n",
    ")\n",
    "\n",
    "# Flatten the `track` struct into individual columns.\n",
    "playlist_track_df = playlist_track_df.select(\n",
    "    functions.col(\"pid\"),\n",
    "    functions.col(\"playlist_name\"),\n",
    "    functions.col(\"num_tracks\"),\n",
    "    functions.col(\"num_albums\"),\n",
    "    functions.col(\"num_followers\"),\n",
    "    functions.col(\"modified_at\"),\n",
    "    functions.col(\"playlist_duration_ms\"),\n",
    "\n",
    "    functions.col(\"track.pos\").alias(\"track_pos\"),\n",
    "    functions.col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "    functions.col(\"track.track_name\").alias(\"track_name\"),\n",
    "    functions.col(\"track.artist_uri\").alias(\"artist_uri\"),\n",
    "    functions.col(\"track.artist_name\").alias(\"artist_name\"),\n",
    "    functions.col(\"track.album_uri\").alias(\"album_uri\"),\n",
    "    functions.col(\"track.album_name\").alias(\"album_name\"),\n",
    "    functions.col(\"track.duration_ms\").alias(\"track_duration_ms\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nSchema of playlist–track table:\")\n",
    "playlist_track_df.printSchema()\n",
    "print(\"\\nExample playlist–track rows:\")\n",
    "playlist_track_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90415a27",
   "metadata": {},
   "source": [
    "### 1.3 Saving flattened DataFrames locally \n",
    "\n",
    "`To not run if one doesn't want to locally save the dataframes`\n",
    "\n",
    "Additionally, to make this cell of code work the user needs to have winutils.exe and hadoop.dll installed locally. This can be found on this github page: \n",
    "\n",
    "- https://github.com/cdarlint/winutils\n",
    "\n",
    "The version which was used for this project was hadoop-3.3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec91d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.3 Persist flattened DataFrames for later tasks (Task 2–5)\n",
    "\n",
    "\"\"\"\n",
    "We now persist the flattened tables to disk so that later tasks (2–5) do not\n",
    "have to re-read and re-flatten the raw JSON.\n",
    "\n",
    "The output folder name encodes the effective dataset size, based on the\n",
    "`NUMBER_OF_SLICES` defined in Section 1.1.\n",
    "\n",
    "Roughly, each slice contains ≈ 1000 playlists, so we name folders like:\n",
    "- \"5k_Playlists\"   → NUMBER_OF_SLICES = 5   (≈ 5,000 playlists)\n",
    "- \"50k_Playlists\"  → NUMBER_OF_SLICES = 50  (≈ 50,000 playlists)\n",
    "\n",
    "If NUMBER_OF_SLICES is large enough to cover all slices (e.g. 1000),\n",
    "we use the folder name \"Full_Playlist\".\n",
    "\n",
    "This keeps the directory structure informative without hard-coding any mode.\n",
    "\"\"\"\n",
    "\n",
    "# NUMBER_OF_SLICES is defined in Section 1.1\n",
    "\n",
    "# Decide folder name based on NUMBER_OF_SLICES\n",
    "# (we approximate 1 slice ≈ 1000 playlists, hence \"Nk_Playlists\")\n",
    "if NUMBER_OF_SLICES >= 1000:\n",
    "    folder_name = \"Full_Playlist\"\n",
    "else:\n",
    "    folder_name = f\"{NUMBER_OF_SLICES}k_Playlists\"\n",
    "\n",
    "post_task1_dir = project_root / \"data_post_Task_1\" / folder_name\n",
    "post_task1_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Post-Task-1 data dir:\", post_task1_dir)\n",
    "\n",
    "# ---- Separate playlist metadata and playlist–track interactions ----\n",
    "\n",
    "# Playlist metadata only (no 'tracks' array)\n",
    "playlists_meta_df = playlists_flat_df.drop(\"tracks\")\n",
    "\n",
    "playlists_out = post_task1_dir / \"playlists_metadata\"\n",
    "playlist_track_out = post_task1_dir / \"playlist_track\"\n",
    "\n",
    "# 1) Save playlist metadata (small, fast)\n",
    "(\n",
    "    playlists_meta_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(str(playlists_out))\n",
    ")\n",
    "\n",
    "# 2) Save playlist–track table (large, but used in all later tasks)\n",
    "(\n",
    "    playlist_track_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(str(playlist_track_out))\n",
    ")\n",
    "\n",
    "print(\"\\nSaved playlist tables to:\")\n",
    "print(\"  \", playlists_out)\n",
    "print(\"  \", playlist_track_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b00328",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b0315",
   "metadata": {},
   "source": [
    "# Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81d6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50933bd8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFOH600_DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
